{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:48:39.264411Z",
     "start_time": "2024-04-28T11:48:39.246453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ],
   "id": "9e2d63c4f2c8b9e2",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:37:56.852819Z",
     "start_time": "2024-04-28T11:37:56.847697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ],
   "id": "f0bef36c6c90e3d2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:37:57.227282Z",
     "start_time": "2024-04-28T11:37:56.855719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "205082fb8e262f07",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:37:57.243958Z",
     "start_time": "2024-04-28T11:37:57.230972Z"
    }
   },
   "source": [
    "\n",
    "# Define the CNN model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:37:57.267496Z",
     "start_time": "2024-04-28T11:37:57.249024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = Net()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ],
   "id": "318c494e506867e9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:37:57.278581Z",
     "start_time": "2024-04-28T11:37:57.269717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "# epochs = 2\n",
    "# for epoch in range(epochs):\n",
    "#     train_loss = 0.0\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data)\n",
    "#         loss = criterion(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()\n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                        100. * batch_idx / len(train_loader), loss.item()))\n",
    "# \n",
    "#     # Evaluate the model on the test set\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             output = model(data)\n",
    "#             test_loss += criterion(output, target).item()\n",
    "#             pred = output.max(1, keepdim=True)[1]\n",
    "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "# \n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "#     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         test_loss, correct, len(test_loader.dataset),\n",
    "#         100. * correct / len(test_loader.dataset)))"
   ],
   "id": "c95cb230932792c3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:37:57.286380Z",
     "start_time": "2024-04-28T11:37:57.282300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "# torch.save(model.state_dict(), 'data/mnist_cnn.pt')"
   ],
   "id": "5b5a3514b216c13c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:37:57.373904Z",
     "start_time": "2024-04-28T11:37:57.289037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load('data/mnist_cnn.pt'))"
   ],
   "id": "114dee5fcca8b678",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Loading Canvas\n",
    "----"
   ],
   "id": "e5b3308577e03c85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:37:57.380871Z",
     "start_time": "2024-04-28T11:37:57.375982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from torchvision.transforms import transforms"
   ],
   "id": "3fbeaa7e978a5c2",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:37:57.391512Z",
     "start_time": "2024-04-28T11:37:57.387036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the canvas size\n",
    "canvas_width = 500\n",
    "canvas_height = 500\n",
    "# Define the preprocessing transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ],
   "id": "d23b3fb818cf6773",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:48:44.129039Z",
     "start_time": "2024-04-28T11:48:44.117339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def load_dataset(save_dir=\"saved_drawings\"):\n",
    "    images = []\n",
    "    labels = []\n",
    "    files = os.listdir(save_dir)\n",
    "    for file in sorted(files):\n",
    "        if file.endswith(\".png\"):\n",
    "            img = cv2.imread(os.path.join(save_dir, file), cv2.IMREAD_GRAYSCALE)\n",
    "            images.append(img)\n",
    "        elif file.endswith(\".txt\"):\n",
    "            with open(os.path.join(save_dir, file), \"r\") as file:\n",
    "                labels.append(int(file.read().strip()))\n",
    "    return images, labels"
   ],
   "id": "6b451576e68482c5",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:48:45.146397Z",
     "start_time": "2024-04-28T11:48:45.138272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_drawing_and_maybe_retrain(canvas, model, save_dir=\"saved_drawings\", retrain_threshold=50):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    file_count = len([name for name in os.listdir(save_dir) if name.endswith('.png')])\n",
    "    file_path = os.path.join(save_dir, f\"drawing_{file_count + 1}.png\")\n",
    "\n",
    "    cv2.imwrite(file_path, canvas)\n",
    "\n",
    "    # Check if it's time to retrain\n",
    "    if (file_count + 1) % retrain_threshold == 0:\n",
    "        print(\"Initiating re-training...\")\n",
    "        train_model(model, save_dir)\n"
   ],
   "id": "e5d9c79f36073759",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:48:47.310601Z",
     "start_time": "2024-04-28T11:48:47.302228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def process_image_for_model(canvas):\n",
    "    \"\"\"\n",
    "    https://discuss.pytorch.org/t/image-processing-functions-on-pytorch-tensor/37203\n",
    "    :param canvas: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # Resize the image to the size expected by the model (e.g., 28x28 for MNIST)\n",
    "    processed_img = cv2.resize(canvas, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Convert to grayscale if the canvas image is in color\n",
    "    if processed_img.shape[2] == 3:  # Check if the image has 3 channels\n",
    "        processed_img = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Invert the image colors (black-on-white to white-on-black)\n",
    "    processed_img = cv2.bitwise_not(processed_img)\n",
    "\n",
    "    # Convert the image to a PyTorch tensor and normalize\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Adjust as per your model's training\n",
    "    ])\n",
    "\n",
    "    return transform(processed_img)"
   ],
   "id": "d64c15ddefecbf39",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:50:28.220274Z",
     "start_time": "2024-04-28T11:50:28.193960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to handle mouse events\n",
    "def draw(event, x, y, flags, param):\n",
    "    global canvas, capturing, model\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        capturing = True\n",
    "\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        capturing = False\n",
    "        # Process the image for prediction\n",
    "        canvas_img = cv2.resize(canvas, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "        canvas_img = cv2.cvtColor(canvas_img, cv2.COLOR_BGR2GRAY)\n",
    "        canvas_img = cv2.bitwise_not(canvas_img)\n",
    "        canvas_tensor = transform(canvas_img)\n",
    "        canvas_tensor = canvas_tensor.unsqueeze(0)\n",
    "\n",
    "        # Perform inference with your CNN model\n",
    "        output = model(canvas_tensor)\n",
    "        prediction = output.max(1, keepdim=True)[1]\n",
    "        pred_text = f\"{prediction.item()}\"\n",
    "\n",
    "        # Process the image to prepare it for the model\n",
    "        canvas_img = process_image_for_model(canvas)\n",
    "        # Save drawing and possibly retrain\n",
    "        save_drawing_and_maybe_retrain(canvas_img, model)\n",
    "\n",
    "        # Clear the canvas\n",
    "        canvas = np.zeros((canvas_height, canvas_width, 3), np.uint8)\n",
    "\n",
    "        # Calculate text size and position it in the center\n",
    "        text_size = cv2.getTextSize(pred_text, cv2.FONT_HERSHEY_SIMPLEX, 2, 2)[0]\n",
    "        text_x = (canvas_width - text_size[0]) // 2\n",
    "        text_y = (canvas_height + text_size[1]) // 2\n",
    "\n",
    "        # Write prediction in red back on the canvas\n",
    "        cv2.putText(canvas, pred_text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    2, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    if capturing:\n",
    "        cv2.circle(canvas, (x, y), 10, (255, 255, 255), -1)"
   ],
   "id": "b197f41afc654dfe",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2024-04-28T11:50:59.100064Z",
     "start_time": "2024-04-28T11:50:59.082583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a window for the canvas\n",
    "cv2.namedWindow(\"Canvas\", cv2.WINDOW_NORMAL)\n",
    "canvas = np.zeros((canvas_height, canvas_width, 3), np.uint8)\n",
    "\n",
    "# Define a flag to indicate when to capture the input\n",
    "capturing = False"
   ],
   "id": "dd8d3e72059852ff",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-28T11:51:02.148394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set the mouse callback function\n",
    "cv2.setMouseCallback(\"Canvas\", draw)\n",
    "\n",
    "while True:\n",
    "    cv2.imshow(\"Canvas\", canvas)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "6352a2060d2547c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T11:49:31.957784Z",
     "start_time": "2024-04-28T11:49:31.957474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DrawingDataset(Dataset):\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        self.data = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.folder_path, self.data[idx])\n",
    "        image = cv2.imread(img_name, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.bitwise_not(image)  # Invert if necessary\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "def train_model(model, folder_path, epochs=1, batch_size=10):\n",
    "    dataset = DrawingDataset(folder_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = torch.nn.CrossEntropyLoss()  # Assuming we have pseudo-labels\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, torch.max(outputs, 1)[1])  # Using pseudo-labeling here\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(\"Training complete.\")\n",
    "    model.eval()\n",
    "\n"
   ],
   "id": "2965e283d188b2bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3e92d4108bda977e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
